
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Turing Tumble Solver (15x17)
",
    "
",
    "This notebook uses BLIP for visual description, converts board to graph, trains GNN, integrates real puzzles, and visualizes everything.
",
    "
",
    "- VLM: BLIP + Optional LLaVA/MiniGPT-4
",
    "- Board: 15x17 grid with device mapping
",
    "- GNN: Trained using symbolic graph
",
    "- Visuals: Board view, GNN output, Graph plot
"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries
",
    "!pip install torch torchvision torchaudio transformers
",
    "!pip install torch-geometric networkx matplotlib scikit-learn
",
    "!pip install -U git+https://github.com/salesforce/BLIP.git
"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP Model and get board description
",
    "from transformers import BlipProcessor, BlipForConditionalGeneration
",
    "from PIL import Image
",
    "image_path = '/content/TT.jpeg'  # Replace with upload code in Colab
",
    "processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
",
    "model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
",
    "image = Image.open(image_path).convert("RGB")
",
    "inputs = processor(image, return_tensors="pt")
",
    "caption_ids = model.generate(**inputs)
",
    "caption = processor.decode(caption_ids[0], skip_special_tokens=True)
",
    "print("BLIP Caption:", caption)"
   ]
  }
 ]
}

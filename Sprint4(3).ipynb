{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bef8bd5e82894ddf97debb278736c14f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7db78509a99c4c37b47f4ed78b6c278f",
              "IPY_MODEL_44a64ddf49044db891a465055cca6803",
              "IPY_MODEL_b87b262d78b54b4d81f6324695572d33"
            ],
            "layout": "IPY_MODEL_14105f8afb814ea7b5dc677de69b53e0"
          }
        },
        "7db78509a99c4c37b47f4ed78b6c278f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e3390c766ee4c7b9c11cd11b26ce38c",
            "placeholder": "​",
            "style": "IPY_MODEL_48b54e377911475a9e19f4678dce278f",
            "value": "Loading checkpoint shards:  50%"
          }
        },
        "44a64ddf49044db891a465055cca6803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7efbaf1d87f94f2598f687a52374e271",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44775675a5e34f2c860dc654196f1827",
            "value": 1
          }
        },
        "b87b262d78b54b4d81f6324695572d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfb916577d85412698a7365769de3f90",
            "placeholder": "​",
            "style": "IPY_MODEL_463198432ddf4a9f9db35ef0d245a1d0",
            "value": " 1/2 [00:49&lt;00:49, 49.30s/it]"
          }
        },
        "14105f8afb814ea7b5dc677de69b53e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e3390c766ee4c7b9c11cd11b26ce38c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48b54e377911475a9e19f4678dce278f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7efbaf1d87f94f2598f687a52374e271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44775675a5e34f2c860dc654196f1827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dfb916577d85412698a7365769de3f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "463198432ddf4a9f9db35ef0d245a1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "source": [
        "!pip uninstall -y torch # Uninstall current torch version to avoid potential conflicts\n",
        "!pip install torch==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118  # Reinstall specific torch version, specify CUDA version if needed\n",
        "!pip install vllm transformers"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OHrEqhlrRgNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import vllm\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoConfig, AutoTokenizer\n",
        "\n",
        "# --- Turing Tumble Hybrid Neuro-Symbolic AI ---\n",
        "\n",
        "# ... (Your existing code for imports, image upload, VLM caption extraction,\n",
        "# logic graph construction, PyTorch Geometric conversion, serialization,\n",
        "# LLM planning prompt, GNN model, and pipeline) ...\n",
        "\n",
        "\n",
        "# --- vLLM Integration with Hugging Face Model ---\n",
        "\n",
        "# Specify the Hugging Face model identifier\n",
        "model_name = \"google/flan-t5-xl\"  # Example: Replace with your desired model\n",
        "\n",
        "# Create the local model directory if it doesn't exist\n",
        "model_dir = \"model_cache\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Download and cache the model configuration and weights\n",
        "config = AutoConfig.from_pretrained(model_name, cache_dir=model_dir)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config, cache_dir=model_dir)\n",
        "\n",
        "# Download and cache the tokenizer (if needed)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_dir)\n",
        "\n",
        "# Set the model path to the cached directory + config file\n",
        "# This assumes the config file name is config.json\n",
        "model_path = os.path.join(model_dir, \"config.json\")\n",
        "\n",
        "\n",
        "# Initialize vLLM, ensuring to set the environment variable before initializing\n",
        "os.environ[\"VLLM_USE_V1\"] = \"0\"  # Force vLLM to use Transformers backend\n",
        "llm = vllm.LLM(model=\"google/flan-t5-xl\")\n",
        "\n",
        "# --- Modified plan_with_llm function ---\n",
        "\n",
        "def plan_with_llm(board_json):\n",
        "    prompt = generate_llm_prompt(board_json)\n",
        "\n",
        "    # Generate response using vLLM\n",
        "    outputs = llm.generate([prompt],\n",
        "                            sampling_params=vllm.SamplingParams(temperature=0.7, max_tokens=128))\n",
        "\n",
        "    # Extract the generated text\n",
        "    response_text = outputs[0].outputs[0].text\n",
        "\n",
        "    # Assuming the response is JSON, parse it\n",
        "    try:\n",
        "        return json.loads(response_text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Warning: LLM response is not valid JSON. Returning empty action.\")\n",
        "        return {}  # Return an empty action if JSON parsing fails\n",
        "\n",
        "# ... (Rest of your code, including hybrid_pipeline and running the pipeline) ...\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "bef8bd5e82894ddf97debb278736c14f",
            "7db78509a99c4c37b47f4ed78b6c278f",
            "44a64ddf49044db891a465055cca6803",
            "b87b262d78b54b4d81f6324695572d33",
            "14105f8afb814ea7b5dc677de69b53e0",
            "7e3390c766ee4c7b9c11cd11b26ce38c",
            "48b54e377911475a9e19f4678dce278f",
            "7efbaf1d87f94f2598f687a52374e271",
            "44775675a5e34f2c860dc654196f1827",
            "dfb916577d85412698a7365769de3f90",
            "463198432ddf4a9f9db35ef0d245a1d0"
          ]
        },
        "id": "7ByIm_tVOPTM",
        "outputId": "0b3da76d-0bd4-48e1-e3cd-afba4b8d29cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bef8bd5e82894ddf97debb278736c14f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "# Turing Tumble Hybrid Neuro-Symbolic AI (Colab Notebook Version)\n",
        "\n",
        "# ------------------------\n",
        "# Install Dependencies (Colab only)\n",
        "# ------------------------\n",
        "!pip install -q transformers torch torchvision torch-geometric networkx openai Pillow\n",
        "!pip install -q openai\n",
        "# ------------------------\n",
        "# Imports\n",
        "# ------------------------\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from torch_geometric.data import Data as GNNData\n",
        "from torch_geometric.nn import GCNConv\n",
        "import networkx as nx\n",
        "from PIL import Image\n",
        "import json\n",
        "import openai\n",
        "import matplotlib.pyplot as plt\n",
        "import os # Import the os module\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "# Set your OpenAI API key using an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"  # Replace YOUR_API_KEY with your actual API key\n",
        "\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        ")\n",
        "\n",
        "print(response.output_text)\n",
        "\n",
        "# ------------------------\n",
        "# Upload Image (Colab)\n",
        "# ------------------------\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]\n",
        "\n",
        "# ------------------------\n",
        "# VLM Caption Extraction (BLIP)\n",
        "# ------------------------\n",
        "def extract_board_description_with_blip(image_path):\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(image, return_tensors=\"pt\")\n",
        "    out = model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# ------------------------\n",
        "# Dummy Logic Graph Construction\n",
        "# ------------------------\n",
        "def build_dummy_graph_from_caption(caption):\n",
        "    G = nx.DiGraph()\n",
        "    G.add_node(0, label=\"ball\", position=[0, 0])\n",
        "    G.add_node(1, label=\"ramp\", position=[1, 0], orientation=\"right\")\n",
        "    G.add_node(2, label=\"gear\", position=[2, 1])\n",
        "    G.add_edges_from([(0, 1), (1, 2)])\n",
        "    return G\n",
        "\n",
        "# ------------------------\n",
        "# Convert to PyTorch Geometric Format\n",
        "# ------------------------\n",
        "def convert_nx_to_gnn_data(graph):\n",
        "    node_labels = [ord(graph.nodes[n]['label'][0]) for n in graph.nodes()]\n",
        "    x = torch.tensor([[l] for l in node_labels], dtype=torch.float)\n",
        "    edge_index = torch.tensor(list(graph.edges())).t().contiguous()\n",
        "    return GNNData(x=x, edge_index=edge_index)\n",
        "\n",
        "# ------------------------\n",
        "# Serialize for LLM\n",
        "# ------------------------\n",
        "def serialize_board_to_json(graph):\n",
        "    nodes = []\n",
        "    for nid, attrs in graph.nodes(data=True):\n",
        "        nodes.append({\n",
        "            \"id\": nid,\n",
        "            \"type\": attrs.get(\"label\", \"unknown\"),\n",
        "            \"position\": attrs.get(\"position\", [0, 0]),\n",
        "            \"orientation\": attrs.get(\"orientation\", \"none\")\n",
        "        })\n",
        "    edges = [{\"from\": u, \"to\": v} for u, v in graph.edges()]\n",
        "    return {\n",
        "        \"nodes\": nodes,\n",
        "        \"connections\": edges,\n",
        "        \"marbles\": [],\n",
        "        \"gear_states\": [],\n",
        "        \"bit_states\": [],\n",
        "        \"goal\": \"Trigger final bit to ON using blue marble\"\n",
        "    }\n",
        "\n",
        "# ------------------------\n",
        "# LLM Planning Prompt\n",
        "# ------------------------\n",
        "def generate_llm_prompt(board_json):\n",
        "    return f\"\"\"\n",
        "You are an AI agent acting as a symbolic planner for a Turing Tumble puzzle.\n",
        "Your goal is: \\\"{board_json['goal']}\\\"\n",
        "\n",
        "Here is the current board state:\n",
        "{json.dumps(board_json, indent=2)}\n",
        "\n",
        "Reply with the best next action using this format:\n",
        "{{\n",
        "  \\\"action\\\": \\\"place_component\\\",\n",
        "  \\\"component\\\": \\\"ramp\\\",\n",
        "  \\\"position\\\": [4, 5],\n",
        "  \\\"orientation\\\": \\\"left\\\"\n",
        "}}\n",
        "\n",
        "Only use these actions:\n",
        "- \\\"place_component\\\"\n",
        "- \\\"remove_component\\\"\n",
        "- \\\"launch_marble\\\"\n",
        "\"\"\"\n",
        "\n",
        "def plan_with_llm(board_json):\n",
        "    prompt = generate_llm_prompt(board_json)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return json.loads(response[\"choices\"][0][\"message\"][\"content\"])\n",
        "\n",
        "# ------------------------\n",
        "# Simple GNN Model\n",
        "# ------------------------\n",
        "class TuringTumbleGNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(1, 16)\n",
        "        self.conv2 = GCNConv(16, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# ------------------------\n",
        "# Pipeline\n",
        "# ------------------------\n",
        "def hybrid_pipeline(image_path):\n",
        "    print(\"[1] Extracting board info with VLM...\")\n",
        "    caption = extract_board_description_with_blip(image_path)\n",
        "    print(\"\\n[Caption]:\", caption)\n",
        "\n",
        "    print(\"[2] Building logic graph from caption...\")\n",
        "    nx_graph = build_dummy_graph_from_caption(caption)\n",
        "\n",
        "    print(\"[3] Serializing for LLM planner...\")\n",
        "    board_json = serialize_board_to_json(nx_graph)\n",
        "\n",
        "    print(\"[4] Invoking LLM planner...\")\n",
        "    action = plan_with_llm(board_json)\n",
        "    print(\"\\n[LLM Suggested Action]:\", action)\n",
        "\n",
        "    print(\"[5] Symbolic reasoning with GNN...\")\n",
        "    gnn_data = convert_nx_to_gnn_data(nx_graph)\n",
        "    model = TuringTumbleGNN()\n",
        "    output = model(gnn_data)\n",
        "    print(\"\\n[GNN Output]:\", output)\n",
        "\n",
        "    return output, action\n",
        "\n",
        "# ------------------------\n",
        "# Run Pipeline\n",
        "# ------------------------\n",
        "output, action = hybrid_pipeline(image_path)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xXQXUpUpOQI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}